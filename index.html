<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WaveC2R</title>
    <!-- Bootstrap -->
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left;
        font-family: "Inter", 'Open Sans', sans-serif;
      }
    </style>

  </head>


    <!-- cover -->
    <section>
      <div class="jumbotron text-center mt-0">
        <div class="container-fluid">
          <div class="row">
            <div class="col">
              <h2 style="font-size:30px;">WaveC2R: Wavelet-Driven Coarse-to-Refined Hierarchical Learning for Radar Retrieval</h2>
              <!-- <h4 style="color:rgb(54, 125, 189);"> arXiv 2025 </h4> -->
              <hr>
                Chunlei Shi</a><sup> 1</sup>&nbsp; &nbsp;
                Han Xu</a><sup> 1</sup>&nbsp; &nbsp;
                Yinghao Li</a><sup> 2</sup>&nbsp; &nbsp;
                Yi-Lin Wei</a><sup> 2</sup>&nbsp; &nbsp;
                Yongchao Feng</a><sup>3 </sup>&nbsp; &nbsp;
                Yecheng Zhang</a><sup>4 </sup>&nbsp; &nbsp;
                Dan Niu</a><sup>1 †</sup>&nbsp; &nbsp;
                <br>
                <br>
              <p>
                <sup>1</sup> School of Automation, Southeast University &nbsp; &nbsp; <br>
                <sup>2</sup> School of Computer Science and Engineering, Sun Yat-sen University &nbsp; &nbsp; <br>
                <sup>3</sup> School of Computer Science and Engineering, Beihang University &nbsp; &nbsp; <br>
                <sup>4</sup> School of Architecture, Tsinghua University &nbsp; &nbsp; <br>
              </p>
              <p>
                <sup>†</sup> corresponding author &nbsp;
                <br>
              </p>
  
              <div class="row justify-content-center">
                <div class="column">
                    <p class="mb-5">
                      <!-- <a class="btn btn-large btn-light" href="https://arxiv.org/pdf/2503.07360" role="button" target="_blank"> -->
                      <a class="btn btn-large btn-light" href="https://arxiv.org/abs/2511.17558" role="button" target="_blank">
                      <i class="fa fa-file"></i> Paper </a>
                    </p>
                </div>
                <div class="column">
                  <p class="mb-5">
                    <a class="btn btn-large btn-light" href="./index.html" role="button" target="_blank">
                    <i class="fa fa-github-alt"></i> Code (Coming soon) </a>
                  </p>
                </div>
                <div class="column">
                  <p class="mb-5">
                    <a class="btn btn-large btn-light" href="https://registry.opendata.aws/sevir/" role="button" target="_blank">
                    <i class="fa fa-file"></i> Dataset </a>
                  </p>
              </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

  <!-- <br> -->

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
              <p class="text-justify" >
                <!-- Language-guided robot dexterous generation enables robots to grasp and manipulate objects based on human commands. 
                However, previous data-driven methods are hard to understand intention and execute grasping with unseen categories 
                in the open set. In this work, we explore a new task, Open-set Language-guided Dexterous Grasp, 
                and find that the main challenge is the huge gap between high-level human language semantics and low-level robot actions. 
                To solve this problem, we propose an Affordance Dexterous Grasp (AffordDexGrasp) framework, 
                with the insight of bridging the gap with a new generalizable-instructive affordance representation. 
                This affordance can generalize to unseen categories by leveraging the object's local structure and category-agnostic semantic attributes, 
                thereby effectively guiding dexterous grasp generation. 
                Built upon the affordance, our framework introduces Affordacne Flow Matching (AFM) for affordance generation with language as input, 
                and Grasp Flow Matching (GFM) for generating dexterous grasp with affordance as input. 
                To evaluate our framework, we build an open-set table-top language-guided dexterous grasp dataset. 
                Extensive experiments in the simulation and real worlds show that our framework surpasses all previous methods in open-set generalization. -->
                Satellite-based radar retrieval methods are widely employed to fill coverage gaps in ground-based radar systems, 
                especially in remote areas affected by terrain blockage and limited detection range. 
                Existing methods predominantly rely on overly simplistic spatial-domain architectures constructed from a single data source, 
                limiting their ability to accurately capture complex precipitation patterns and sharply defined meteorological boundaries.
                To address these limitations, we propose WaveC2R, a novel wavelet-driven coarse-to-refined framework for radar retrieval. 
                WaveC2R integrates complementary multi-source data and leverages frequency-domain decomposition to separately model low-frequency components for capturing precipitation patterns and high-frequency components for delineating sharply defined meteorological boundaries. 
                Specifically, WaveC2R consists of two stages (i) Intensity-Boundary Decoupled Learning, 
                which leverages wavelet decomposition and frequency-specific loss functions to separately optimize low-frequency intensity and high-frequency boundaries; 
                and (ii) Detail-Enhanced Diffusion Refinement, which employs frequency-aware conditional priors and multi-source data to progressively enhance fine-scale precipitation structures while preserving coarse-scale meteorological consistency.
                Experimental results on the publicly available SEVIR dataset demonstrate that WaveC2R achieves state-of-the-art performance in satellite-based radar retrieval, particularly excelling at preserving high-intensity precipitation features and sharply defined meteorological boundaries. Our project is available at https://spring-lovely.github.io/WaveC2R/.
              </p>
              <!-- <div class="row justify-content-center" style="align-items:center; display:flex;"></div>
                <img src="images/task.png" alt="input" class="img-responsive graph" width="100%"/>
              </div> -->
            <br>
        </div>
      </div>
    </div>
  </section>
  <!-- <br>
  <section class="video-section">
    <video class="video-container" src="images/video_mambaRain.mp4" autoplay muted controls>
      Your browser does not support the video tag.
    </video>
  </section>
  <br> -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>DexGYSNet Construction</strong></h2>
          <hr style="margin-top:0px">
          <p class="text-justify">
            The DexGYSNet dataset is constructed in a cost-effective manner by exploiting human grasp behavior and the extensive capabilities of Large Language Models (LLM).  
            We develop the Hand-Object Interaction Retargeting (HOIR) strategy to transform human grasps into dexterous grasps with high quality and hand-object interaction consistency. 
            Then, we implement an LLM-assisted Language Guidance Annotation system, which leverages the knowledge of Large Language Models (LLM) to produce flexible 
            and fine-grained annotations for language guidance.
          </p>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/construction.png" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br> -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>WaveC2R Framework</strong></h2>
          <hr style="margin-top:0px">
          <p class="text-justify">
          <!-- The pipeline of Affordance Dexterous Grasp framework. 
          The inference pipeline includes three stages: 1) intention pre-understanding assisted by MLLM; 
          2) affordance flow matching for generating affordance base on MLLM ouput; 
          3) Grasp Flow Matching and Optimization for outputing grasp poses based on the affordance and MLLM outputs. 
          In the training time, AFM and GFM are independently trained one after another. 
          Transformer and Perceiver are attention-based interaction module for velocity vector field prediction. -->
          Overall architecture of the proposed WaveC2R framework. 
          (a) Stage I: Intensity-Boundary Decoupled Learning employs the Wavelet-Temporal-Frequency (WTF) module 
          to extract hierarchical frequency-domain features from multi-source satellite observations and generates coarse radar 
          estimates through frequency-decomposed optimization. (b) Stage II: Detail-Enhanced Diffusion Refinement progressively 
          refines coarse estimates via conditional diffusion with physics-aware frequency-decomposed priors. 
          (c) WTF Block demonstrates the core wavelet-based attention mechanism that performs temporal-frequency feature fusion 
          across multiple scales.
          </p>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/WaveC2R_ov.jpg" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Experiment</strong></h2>
          <hr style="margin-top:0px">
          <p class="text-justify">
          Qualitative comparison of radar reflectivity predictions for four heavy precipitation events from the SEVIR dataset. 
          Red boxes highlight magnified convective regions where our WaveC2R demonstrates superior intensity accuracy and boundary sharpness 
          compared to competing methods. Input modalities include visible, dual-channel infrared, and lightning observations.
          </p>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/fig_vis1.jpg" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Real World Experiment</strong></h2>
          <hr style="margin-top:0px">
          <div class="row">
            <div class="col-md-6">
              <img src="images/realword_setting.png" alt="Real World Experiment Setup" class="img-responsive" width="100%" />
            </div>
            <div class="col-md-6">
              <h3>Experiment Setup</h3>
              <p>
                The real-word experiments are conducted to verify the simulation-to-reality ability of our framework. 
                We employ a Leap Hand, a Kinova Gen3 6DOF arms and an original wrist RGB-D camera of Kinova arm. 
                In experiment, we synthesize the scene point cloud by taking several partial depth maps around the object. 
                Then the scene point cloud, a RGB image and the user language command are fed into our framework to obtain the dexterous grasp pose. 
                During execution, we first move the the arm to a pre-grasp position, 
                then synchronously move the joints of the robotic arm and the dexterous hand to reach the target pose. 
              </p>
            </div>
            <h3>Experiment Visualization</h3>
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/sim_vis.png" alt="input" class="img-responsive graph" width="100%"/>
            </div> -->
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/realword_vis.png" alt="input" class="img-responsive graph" width="100%"/>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> --> -->
  <!-- Contact -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Contact</strong></h2>
          <hr style="margin-top:0px">
          <p>If you have any questions, please feel free to contact us:
            <ul>
              <li><b>Chunlei Shi</b>&colon; 230238514<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>seu.edu.cn </li>
            </ul>
          </p>
      </div>
    </div>
  </div>
  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']],
      macros: {
        bm: ["{\\boldsymbol #1}",1],
      }}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
